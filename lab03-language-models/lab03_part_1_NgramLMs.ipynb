{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDWi0xoTFxHZ"
      },
      "source": [
        "# Lab 3, part 1: N-gram Language Models\n",
        "\n",
        "Welcome to the third session of the NLP course. Today we will explore different approaches for Language Models. This session is divided in two parts. First, we will look at some classic approaches known as N-gram models. In the second part, we will use neural networks to model corpus text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbsta7lIHhz"
      },
      "source": [
        "## Download WikiText-2 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.265129Z",
          "start_time": "2020-01-29T18:30:17.460952Z"
        },
        "id": "jfGVtATnIHh2",
        "outputId": "8d4bba96-aa76-4d93-a94e-86a19db5a80f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-30 12:02:31--  https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10797148 (10M) [text/plain]\n",
            "Saving to: ‘train.txt’\n",
            "\n",
            "\rtrain.txt             0%[                    ]       0  --.-KB/s               \rtrain.txt           100%[===================>]  10.30M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-01-30 12:02:32 (90.2 MB/s) - ‘train.txt’ saved [10797148/10797148]\n",
            "\n",
            "--2025-01-30 12:02:32--  https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/valid.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1121681 (1.1M) [text/plain]\n",
            "Saving to: ‘valid.txt’\n",
            "\n",
            "valid.txt           100%[===================>]   1.07M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-01-30 12:02:32 (18.2 MB/s) - ‘valid.txt’ saved [1121681/1121681]\n",
            "\n",
            "--2025-01-30 12:02:32--  https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1256449 (1.2M) [text/plain]\n",
            "Saving to: ‘test.txt’\n",
            "\n",
            "test.txt            100%[===================>]   1.20M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-01-30 12:02:32 (18.3 MB/s) - ‘test.txt’ saved [1256449/1256449]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/train.txt\n",
        "! wget https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/valid.txt\n",
        "! wget https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/test.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW7UOlBNIHh5"
      },
      "source": [
        "## Helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.271133Z",
          "start_time": "2020-01-29T18:30:18.266713Z"
        },
        "id": "u5edQma8IHh7"
      },
      "outputs": [],
      "source": [
        "from collections import Counter, defaultdict\n",
        "import math\n",
        "import copy\n",
        "import random\n",
        "import operator\n",
        "\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "\n",
        "# some helper functions\n",
        "def prepare_data(filename):\n",
        "    data = [l.strip().lower().split() + ['</s>'] for l in open(filename) if l.strip()]\n",
        "    corpus = flatten(data)\n",
        "    vocab = set(corpus)\n",
        "    return vocab, data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvjT2fTlIHh9"
      },
      "source": [
        "## N-Gram LM\n",
        "A language model assigns a probability to each possible next word given a history of previous words (context).\n",
        "\n",
        "$P(w_t|w_{t-1}, w_{t-2}, ... , w_1)$\n",
        "\n",
        "### Markov Assumption\n",
        "Since calculating the probability of the whole sentence is not feasible, the Markov Assumption is introduced.\n",
        "\n",
        "It assumes that each next word only depends on the previous K words (in an N-Gram language model, K = N-1).\n",
        "- Unigram: $P(w_t|w_{t-1}, w_{t-2}, ... , w_1) = P(w_t)$\n",
        "- Bigram: $P(w_t|w_{t-1}, w_{t-2}, ... , w_1) = P(w_t|w_{t-1}) $\n",
        "- Trigram: $P(w_t|w_{t-1}, w_{t-2}, ... , w_1) = P(w_t|w_{t-1}, w_{t-2})$\n",
        "\n",
        "For an N-Gram language model, the probability is calculated by counting the frequency:\n",
        "\n",
        "$P(w_t|w_{t-1}, w_{t-2}, ... ,w_{t-N+1}) = \\frac{C(w_t, w_{t-1}, w_{t-2}, ... ,w_{t-N+1} )}{C(w_{t-1}, w_{t-2}, ... ,w_{t-N+1})}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.290242Z",
          "start_time": "2020-01-29T18:30:18.272506Z"
        },
        "id": "Ql01Z8IiIHh_"
      },
      "outputs": [],
      "source": [
        "class NGramLM():\n",
        "    def __init__(self, N):\n",
        "        self.N = N\n",
        "        self.vocab = set()\n",
        "        self.data = []\n",
        "        self.prob = {}\n",
        "        self.counts = defaultdict(Counter)\n",
        "\n",
        "    # For N = 1, the probability is stored in a dict   P = prob[next_word]\n",
        "    # For N > 1, the probability is in a nested dict   P = prob[context][next_word]\n",
        "    def train(self, vocab, data, smoothing_k=0):\n",
        "        self.vocab = vocab\n",
        "        self.data = data\n",
        "        self.smoothing_k = smoothing_k\n",
        "\n",
        "        if self.N == 1:\n",
        "            self.counts = Counter(flatten(data))\n",
        "            self.prob = self.get_prob(self.counts)\n",
        "        else:\n",
        "            self.vocab.add('<s>')\n",
        "            counts = self.count_ngram()\n",
        "\n",
        "            self.prob = {}\n",
        "            for context, counter in counts.items():\n",
        "                self.prob[context] = self.get_prob(counter)\n",
        "\n",
        "    def count_ngram(self):\n",
        "        counts = defaultdict(Counter)\n",
        "        for sentence in self.data:\n",
        "            sentence = (self.N - 1) * ['<s>'] + sentence\n",
        "            for i in range(len(sentence)-self.N+1):\n",
        "                context = sentence[i:i+self.N-1]\n",
        "                context = \" \".join(context)\n",
        "                word = sentence[i+self.N-1]\n",
        "                counts[context][word] += 1\n",
        "\n",
        "        self.counts = counts\n",
        "        return counts\n",
        "\n",
        "    # normalize counts into probability(considering smoothing)\n",
        "    def get_prob(self, counter):\n",
        "        total = float(sum(counter.values()))\n",
        "        k = self.smoothing_k\n",
        "\n",
        "        prob = {}\n",
        "        for word, count in counter.items():\n",
        "            prob[word] = (count + k) / (total + len(self.vocab) * k)\n",
        "        return prob\n",
        "\n",
        "    def get_ngram_logprob(self, word, seq_len, context=\"\"):\n",
        "        if self.N == 1 and word in self.prob.keys():\n",
        "            return math.log(self.prob[word]) / seq_len\n",
        "        elif self.N > 1 and not self._is_unseen_ngram(context, word):\n",
        "            return math.log(self.prob[context][word]) / seq_len\n",
        "        else:\n",
        "            # assign a small probability to the unseen ngram\n",
        "            # to avoid log of zero and to penalise unseen word or context\n",
        "            return math.log(1/len(self.vocab)) / seq_len\n",
        "\n",
        "    def get_ngram_prob(self, word, context=\"\"):\n",
        "        if self.N == 1 and word in self.prob.keys():\n",
        "            return self.prob[word]\n",
        "        elif self.N > 1 and not self._is_unseen_ngram(context, word):\n",
        "            return self.prob[context][word]\n",
        "        elif word in self.vocab and self.smoothing_k > 0:\n",
        "            # probability assigned by smoothing\n",
        "            return self.smoothing_k / (sum(self.counts[context].values()) + self.smoothing_k*len(self.vocab))\n",
        "        else:\n",
        "            # unseen word or context\n",
        "            return 0\n",
        "\n",
        "    # In this method, the perplexity is measured at the sentence-level, averaging over all sentences.\n",
        "    # Actually, it is also possible to calculate perplexity by merging all sentences into a long one.\n",
        "    def perplexity(self, test_data):\n",
        "        log_ppl = 0\n",
        "        if self.N == 1:\n",
        "            for sentence in test_data:\n",
        "                for word in sentence:\n",
        "                    log_ppl += self.get_ngram_logprob(word=word, seq_len=len(sentence))\n",
        "        else:\n",
        "            for sentence in test_data:\n",
        "                for i in range(len(sentence)-self.N+1):\n",
        "                    context = sentence[i:i+self.N-1]\n",
        "                    context = \" \".join(context)\n",
        "                    word = sentence[i+self.N-1]\n",
        "                    log_ppl += self.get_ngram_logprob(context=context, word=word, seq_len=len(sentence))\n",
        "\n",
        "        log_ppl /= len(test_data)\n",
        "        ppl = math.exp(-log_ppl)\n",
        "        return ppl\n",
        "\n",
        "    def _is_unseen_ngram(self, context, word):\n",
        "        if context not in self.prob.keys() or word not in self.prob[context].keys():\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    # generate the most probable k words\n",
        "    def generate_next(self, context, k):\n",
        "        context = (self.N-1) * '<s> ' + context\n",
        "        context = context.split()\n",
        "        ngram_context_list = context[-self.N+1:]\n",
        "        ngram_context = \" \".join(ngram_context_list)\n",
        "\n",
        "        if ngram_context in self.prob.keys():\n",
        "            candidates = self.prob[ngram_context]\n",
        "            most_probable_words = sorted(candidates.items(), key=lambda kv: kv[1], reverse=True)\n",
        "            for i in range(min(k, len(most_probable_words))):\n",
        "                print(\" \".join(context[self.N-1:])+\" \"+most_probable_words[i][0]+\"\\t P={}\".format(most_probable_words[i][1]))\n",
        "        else:\n",
        "            print(\"Unseen context!\")\n",
        "\n",
        "    # generate the next n words with greedy search\n",
        "    def generate_next_n(self, context, n):\n",
        "        context = (self.N-1) * '<s> ' + context\n",
        "        context = context.split()\n",
        "        ngram_context_list = context[-self.N+1:]\n",
        "        ngram_context = \" \".join(ngram_context_list)\n",
        "\n",
        "        for i in range(n):\n",
        "            try:\n",
        "                candidates = self.prob[ngram_context]\n",
        "                most_likely_next = max(candidates.items(), key=operator.itemgetter(1))[0]\n",
        "                context += [most_likely_next]\n",
        "                ngram_context_list = ngram_context_list[1:] + [most_likely_next]\n",
        "                ngram_context = \" \".join(ngram_context_list)\n",
        "            except:\n",
        "                break\n",
        "        print(\" \".join(context[self.N-1:]))\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VwN9CXyIHiC"
      },
      "source": [
        "## Train with toy dataset\n",
        "\n",
        "At this step, let's train a Bigram language model on the toy dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.297355Z",
          "start_time": "2020-01-29T18:30:18.291387Z"
        },
        "id": "sabDGGntIHiC",
        "outputId": "729bb249-224b-4126-fc66-3c2d65f8e65d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['i', 'like', 'ice', 'cream', '</s>'], ['i', 'like', 'chocolate', '</s>'], ['i', 'hate', 'beans', '</s>']]\n",
            "{'like', 'ice', 'i', 'cream', 'hate', 'beans', '</s>', 'chocolate'}\n"
          ]
        }
      ],
      "source": [
        "corpus = [\"I like ice cream\",\n",
        "         \"I like chocolate\",\n",
        "         \"I hate beans\"]\n",
        "data = [l.strip().lower().split() + ['</s>'] for l in corpus if l.strip()]\n",
        "vocab = set(flatten(data))\n",
        "print(data)\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.301895Z",
          "start_time": "2020-01-29T18:30:18.298317Z"
        },
        "id": "-HKqP6zvIHiE"
      },
      "outputs": [],
      "source": [
        "def print_probability(lm):\n",
        "    for context in lm.vocab:\n",
        "        for word in lm.vocab:\n",
        "            prob = lm.get_ngram_prob(word, context)\n",
        "            print(\"P({}\\t|{}) = {}\".format(word, context, prob))\n",
        "        print(\"--------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y-jHF6bIHiF"
      },
      "source": [
        "## Smoothing\n",
        "The smoothing approach is used to deal with the sparsity problem in the N-Gram LM.\n",
        "The probability mass is shifted towards the less frequent words.\n",
        "\n",
        "For example, with an add-1 smoothing, the probability is calculated as:\n",
        "\n",
        "$$P(w_t | context) = \\frac{C(w_t, context)+1}{C(context)+|V|}$$\n",
        "\n",
        "Q1: What is the disadvantage of smoothing?\n",
        "\n",
        "A: If there are too many word with zero counts, the frequent words will sacrifice more probability, which might lead to higher perplexity on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.310269Z",
          "start_time": "2020-01-29T18:30:18.302761Z"
        },
        "id": "nBS-MIWiIHiG",
        "outputId": "a52e4bcb-505a-407c-9fa2-44286136152c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(like\t|like) = 0.09090909090909091\n",
            "P(ice\t|like) = 0.18181818181818182\n",
            "P(i\t|like) = 0.09090909090909091\n",
            "P(cream\t|like) = 0.09090909090909091\n",
            "P(<s>\t|like) = 0.09090909090909091\n",
            "P(hate\t|like) = 0.09090909090909091\n",
            "P(beans\t|like) = 0.09090909090909091\n",
            "P(</s>\t|like) = 0.09090909090909091\n",
            "P(chocolate\t|like) = 0.18181818181818182\n",
            "--------------------------\n",
            "P(like\t|ice) = 0.1\n",
            "P(ice\t|ice) = 0.1\n",
            "P(i\t|ice) = 0.1\n",
            "P(cream\t|ice) = 0.2\n",
            "P(<s>\t|ice) = 0.1\n",
            "P(hate\t|ice) = 0.1\n",
            "P(beans\t|ice) = 0.1\n",
            "P(</s>\t|ice) = 0.1\n",
            "P(chocolate\t|ice) = 0.1\n",
            "--------------------------\n",
            "P(like\t|i) = 0.25\n",
            "P(ice\t|i) = 0.08333333333333333\n",
            "P(i\t|i) = 0.08333333333333333\n",
            "P(cream\t|i) = 0.08333333333333333\n",
            "P(<s>\t|i) = 0.08333333333333333\n",
            "P(hate\t|i) = 0.16666666666666666\n",
            "P(beans\t|i) = 0.08333333333333333\n",
            "P(</s>\t|i) = 0.08333333333333333\n",
            "P(chocolate\t|i) = 0.08333333333333333\n",
            "--------------------------\n",
            "P(like\t|cream) = 0.1\n",
            "P(ice\t|cream) = 0.1\n",
            "P(i\t|cream) = 0.1\n",
            "P(cream\t|cream) = 0.1\n",
            "P(<s>\t|cream) = 0.1\n",
            "P(hate\t|cream) = 0.1\n",
            "P(beans\t|cream) = 0.1\n",
            "P(</s>\t|cream) = 0.2\n",
            "P(chocolate\t|cream) = 0.1\n",
            "--------------------------\n",
            "P(like\t|<s>) = 0.08333333333333333\n",
            "P(ice\t|<s>) = 0.08333333333333333\n",
            "P(i\t|<s>) = 0.3333333333333333\n",
            "P(cream\t|<s>) = 0.08333333333333333\n",
            "P(<s>\t|<s>) = 0.08333333333333333\n",
            "P(hate\t|<s>) = 0.08333333333333333\n",
            "P(beans\t|<s>) = 0.08333333333333333\n",
            "P(</s>\t|<s>) = 0.08333333333333333\n",
            "P(chocolate\t|<s>) = 0.08333333333333333\n",
            "--------------------------\n",
            "P(like\t|hate) = 0.1\n",
            "P(ice\t|hate) = 0.1\n",
            "P(i\t|hate) = 0.1\n",
            "P(cream\t|hate) = 0.1\n",
            "P(<s>\t|hate) = 0.1\n",
            "P(hate\t|hate) = 0.1\n",
            "P(beans\t|hate) = 0.2\n",
            "P(</s>\t|hate) = 0.1\n",
            "P(chocolate\t|hate) = 0.1\n",
            "--------------------------\n",
            "P(like\t|beans) = 0.1\n",
            "P(ice\t|beans) = 0.1\n",
            "P(i\t|beans) = 0.1\n",
            "P(cream\t|beans) = 0.1\n",
            "P(<s>\t|beans) = 0.1\n",
            "P(hate\t|beans) = 0.1\n",
            "P(beans\t|beans) = 0.1\n",
            "P(</s>\t|beans) = 0.2\n",
            "P(chocolate\t|beans) = 0.1\n",
            "--------------------------\n",
            "P(like\t|</s>) = 0.1111111111111111\n",
            "P(ice\t|</s>) = 0.1111111111111111\n",
            "P(i\t|</s>) = 0.1111111111111111\n",
            "P(cream\t|</s>) = 0.1111111111111111\n",
            "P(<s>\t|</s>) = 0.1111111111111111\n",
            "P(hate\t|</s>) = 0.1111111111111111\n",
            "P(beans\t|</s>) = 0.1111111111111111\n",
            "P(</s>\t|</s>) = 0.1111111111111111\n",
            "P(chocolate\t|</s>) = 0.1111111111111111\n",
            "--------------------------\n",
            "P(like\t|chocolate) = 0.1\n",
            "P(ice\t|chocolate) = 0.1\n",
            "P(i\t|chocolate) = 0.1\n",
            "P(cream\t|chocolate) = 0.1\n",
            "P(<s>\t|chocolate) = 0.1\n",
            "P(hate\t|chocolate) = 0.1\n",
            "P(beans\t|chocolate) = 0.1\n",
            "P(</s>\t|chocolate) = 0.2\n",
            "P(chocolate\t|chocolate) = 0.1\n",
            "--------------------------\n"
          ]
        }
      ],
      "source": [
        "lm = NGramLM(2)\n",
        "lm.train(vocab, data, smoothing_k=0)\n",
        "\n",
        "######################################################\n",
        "# Q2: try with add-1 smoothing and see the probability\n",
        "######################################################\n",
        "lm.train(vocab, data, smoothing_k=1)\n",
        "\n",
        "print_probability(lm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG35kkhVIHiH"
      },
      "source": [
        "## Train on WikiText-2 dataset and evaluate perplexity\n",
        "### Evaluating perplexity\n",
        "\n",
        "Q3: Why do we need to calculate log perplexity?\n",
        "\n",
        "A: $P(w_1, w_2, \\dots, w_n) = P(w_1) \\cdot P(w_2) \\cdot \\dots \\cdot P(w_n)$. Repeated multiplication of probabilities could lead to underflow. Taking the log replaces this multiplication with addition, mitigating this problem.\n",
        "\n",
        "$ PPL(W) = P(w_1, w_2, ... , w_n)^{-\\frac{1}{n}}$\n",
        "\n",
        "$ log(PPL(W)) = -\\frac{1}{n}\\sum^n_{k=1}log(P(w_k|w_1, w_2, ... , w_{k-1}))$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.712903Z",
          "start_time": "2020-01-29T18:30:18.312677Z"
        },
        "id": "sWJEjssXIHiI",
        "outputId": "dea43927-74fe-4437-be0f-ddb541e299be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28912\n"
          ]
        }
      ],
      "source": [
        "vocab, train_data = prepare_data('train.txt')\n",
        "_, valid_data = prepare_data('valid.txt')\n",
        "_, test_data = prepare_data('test.txt')\n",
        "print(len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:22.224402Z",
          "start_time": "2020-01-29T18:30:18.714081Z"
        },
        "id": "rYcGOj6cIHiI"
      },
      "outputs": [],
      "source": [
        "lm = NGramLM(3)\n",
        "lm.train(vocab, train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:22.811004Z",
          "start_time": "2020-01-29T18:30:22.225566Z"
        },
        "id": "Qh0NmO16IHiI",
        "outputId": "1f63efcc-e222-4dfa-efdd-114fa642f9e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "387.1190374834364\n",
            "310.6880368161233\n"
          ]
        }
      ],
      "source": [
        "print(lm.perplexity(valid_data))\n",
        "print(lm.perplexity(test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw9EMl6XIHiK"
      },
      "source": [
        "## Generating sentences\n",
        "With a pre-trained N-Gram language model, we can predict possible next words given context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:22.815241Z",
          "start_time": "2020-01-29T18:30:22.812141Z"
        },
        "id": "LVE1LyfxIHiL",
        "outputId": "cd527f56-fe69-482b-d7cd-cf9750ee150d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the eggs hatch at night , and the larvae swim to the water surface where they drift with the ocean currents , preying on <unk> . this stage involves three <unk> and lasts for 15 – 35 days . after the third moult , the juvenile takes on a form closer to the adult , and adopts a <unk> lifestyle . the juveniles are rarely seen in the wild , and are poorly known , although they are known to be capable of digging extensive burrows . it is estimated that only 1 larva in every 20 @,@ 000 survives to the <unk> phase . when they reach a carapace length of 15 mm ( 0 @.@ 59 in ) , the juveniles leave their burrows and start their adult lives . </s>\n",
            "the eggs hatch at night , and the larvae swim to the water surface where they were\t P=0.12408759124087591\n",
            "the eggs hatch at night , and the larvae swim to the water surface where they are\t P=0.08029197080291971\n",
            "the eggs hatch at night , and the larvae swim to the water surface where they would\t P=0.051094890510948905\n"
          ]
        }
      ],
      "source": [
        "# generate the most probable following words given the context\n",
        "print(\" \".join(valid_data[12]))\n",
        "\n",
        "# actually the only useful context in the Trigram language model is [\"where\", \"they\"]\n",
        "context = \"the eggs hatch at night , and the larvae swim to the water surface where they\"\n",
        "lm.generate_next(context, 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "e1y3MW9DZ-nb",
        "outputId": "407c20be-c67b-4252-8fe8-240a311a1da7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the eggs hatch\t P=0.18181818181818182\n",
            "the eggs of\t P=0.18181818181818182\n",
            "the eggs are\t P=0.13636363636363635\n",
            "---\n",
            "the first\t P=0.03398926654740608\n",
            "the <unk>\t P=0.020274299344066785\n",
            "the episode\t P=0.015802027429934407\n",
            "---\n",
            " =\t P=0.26132873311734756\n",
            " the\t P=0.14112004039214035\n",
            " in\t P=0.06353347077881095\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "# we can also generate with shorter contexts, even shorter than N-1\n",
        "\n",
        "contexts = [\"the eggs\",\n",
        "            \"the\",\n",
        "            \"\"]\n",
        "for context in contexts:\n",
        "  lm.generate_next(context, 3)\n",
        "  print(\"---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:22.828378Z",
          "start_time": "2020-01-29T18:30:22.816839Z"
        },
        "id": "sIiRSWPoIHiM",
        "outputId": "906516a4-f00a-4415-d52a-52a822bf99b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the eggs hatch at night , and the larvae swim to the water surface where they were not able to get the part of the <unk>\n",
            "the eggs hatch at night , and the larvae swim to the water surface where they were not able to get the part of the <unk> of the <unk> of the <unk> of the <unk> of\n"
          ]
        }
      ],
      "source": [
        "context = \"the eggs hatch at night , and the larvae swim to the water surface where they\"\n",
        "\n",
        "# generate the next n words with greedy search\n",
        "lm.generate_next_n(context, 10)\n",
        "\n",
        "# This is not a good method in practice,\n",
        "# because wrong predictions in the early steps would introduce errors to the following predictions\n",
        "lm.generate_next_n(context, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSEVmuouIHiN"
      },
      "source": [
        "## Effect of N\n",
        "\n",
        "Q4: Why does the perplexity increases on the validation and test data when N is large?\n",
        "\n",
        "A: For large N, there is a chance that the context wasn't encountered in the training data, or that the context was encountered infrequently. Thus at the time of inference, the model has a lower probability of predicting a word that might actually occur after the given context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:41.930826Z",
          "start_time": "2020-01-29T18:30:22.829892Z"
        },
        "id": "wLVeKE_JIHiN",
        "outputId": "7730a3b8-c549-494d-f097-031afa392f9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************************\n",
            "1-gram LM perplexity on train set: 708.9625836202729\n",
            "1-gram LM perplexity on valid set: 599.6450225274759\n",
            "1-gram LM perplexity on test  set: 553.0467265986567\n",
            "************************\n",
            "2-gram LM perplexity on train set: 48.23277624725139\n",
            "2-gram LM perplexity on valid set: 130.75949726991627\n",
            "2-gram LM perplexity on test  set: 114.76592406312065\n",
            "************************\n",
            "3-gram LM perplexity on train set: 5.996757661765895\n",
            "3-gram LM perplexity on valid set: 387.1190374834364\n",
            "3-gram LM perplexity on test  set: 310.6880368161233\n",
            "************************\n",
            "4-gram LM perplexity on train set: 1.7630463270256491\n",
            "4-gram LM perplexity on valid set: 1091.9348578736633\n",
            "4-gram LM perplexity on test  set: 846.7563102382584\n",
            "************************\n",
            "5-gram LM perplexity on train set: 1.147830619188205\n",
            "5-gram LM perplexity on valid set: 1558.823545905558\n",
            "5-gram LM perplexity on test  set: 1201.746345143052\n"
          ]
        }
      ],
      "source": [
        "for n in range(1,6):\n",
        "    lm = NGramLM(n)\n",
        "    lm.train(vocab, train_data)\n",
        "    print(\"************************\")\n",
        "    print(\"{}-gram LM perplexity on train set: {}\".format(n, lm.perplexity(train_data)))\n",
        "    print(\"{}-gram LM perplexity on valid set: {}\".format(n, lm.perplexity(valid_data)))\n",
        "    print(\"{}-gram LM perplexity on test  set: {}\".format(n, lm.perplexity(test_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgM5mePhIHiO"
      },
      "source": [
        "## Interpolation\n",
        "In interpolation, we mix the probability estimates from all the n-gram estimators to alleviate the sparsity problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:41.937643Z",
          "start_time": "2020-01-29T18:30:41.932047Z"
        },
        "id": "5f57uTZvIHiO"
      },
      "outputs": [],
      "source": [
        "class InterpolateNGramLM(NGramLM):\n",
        "\n",
        "    def __init__(self, N):\n",
        "        super(InterpolateNGramLM, self).__init__(N)\n",
        "        self.ngram_lms = []\n",
        "        self.lambdas = []\n",
        "\n",
        "    def train(self, vocab, data, smoothing_k=0, lambdas=[]):\n",
        "        assert len(lambdas) == self.N\n",
        "        assert sum(lambdas) - 1 < 1e-9\n",
        "        self.vocab = vocab\n",
        "        self.lambdas = lambdas\n",
        "\n",
        "        for i in range(self.N, 0, -1):\n",
        "            lm = NGramLM(i)\n",
        "            print(\"Training {}-gram language model\".format(i))\n",
        "            lm.train(vocab, data, smoothing_k)\n",
        "            self.ngram_lms.append(lm)\n",
        "\n",
        "    def get_ngram_logprob(self, word, seq_len, context):\n",
        "        prob = 0\n",
        "        for i, (coef, lm) in enumerate(zip(self.lambdas, self.ngram_lms)):\n",
        "            context_words = context.split()\n",
        "            cutted_context = \" \".join(context_words[-self.N + i + 1:])\n",
        "            prob += coef * lm.get_ngram_prob(context=cutted_context, word=word)\n",
        "        return math.log(prob) / seq_len\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:46.507705Z",
          "start_time": "2020-01-29T18:30:41.939076Z"
        },
        "id": "6TafZ7KpIHiO",
        "outputId": "af274f98-a39a-498a-ec6b-de82b4cd6021",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training 3-gram language model\n",
            "Training 2-gram language model\n",
            "Training 1-gram language model\n"
          ]
        }
      ],
      "source": [
        "###################################################\n",
        "# Q5: tune your coefficients to decrease perplexity\n",
        "###################################################\n",
        "ilm = InterpolateNGramLM(3)\n",
        "ilm.train(vocab, train_data, lambdas=[0.5, 0.4, 0.1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:48.376911Z",
          "start_time": "2020-01-29T18:30:46.509150Z"
        },
        "id": "UZGf_gQOIHiP",
        "outputId": "5a243f5a-e7cb-4dbf-c26e-6cd2acfd3aa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "126.26886448793077\n",
            "103.0773276220259\n"
          ]
        }
      ],
      "source": [
        "print(ilm.perplexity(valid_data))\n",
        "print(ilm.perplexity(test_data))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "lab03_1_NgramLMs.ok.ipynb",
      "provenance": []
    },
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit ('py38_pytorch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "version": 3,
    "vscode": {
      "interpreter": {
        "hash": "ff27035c8dc0a26468b79942def09685664b2e815f4bca7e9395dcaceb48c986"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}